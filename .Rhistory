y_test <- to_categorical(g_test, num_classes)
x_train
g_train
table(g_train)
y_train
g_train
summary(x_train)
x_train[1]
x_train[0]
x_train[2]
x_train$1
x_train
plot(x_train[1])
x_train
plot_grayscale(x_train[1,,])
p1 <- plot_grayscale(x_train[1,,])
dim(x_train)
p1 <- plot_grayscale(x_train[1,,,])
plot(p1)
p1 <- plot_grayscale(x_train[1,,,])
p1 <- plot_grayscale(x_train[1,,,])
p2 <- plot_grayscale(x_train[2,,,])
p3 <- plot_grayscale(x_train[3,,,])
p4 <- plot_grayscale(x_train[4,,,])
p5 <- plot_grayscale(x_train[5,,,])
p6 <- plot_grayscale(x_train[6,,,])
plot_grid(p1, p2, p3)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
model_lr <- keras_model_sequential() %>%
layer_dense(input_shape = num_pixels,
units = num_classes,
activation = "softmax")
table(g_train)
model_lr <- keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = num_classes, activation = "softmax")
model_lr %>%                                    # note: modifying model_lr in place
compile(loss = "categorical_crossentropy",   # which loss to use
optimizer = optimizer_adam(),     # how to optimize the loss
metrics = c("accuracy"))
summary(model_lr)
p1
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2, labels = c("pp"))
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2, labels = c(fashion-mnist-labels[1]))
fashion_mnist-labels[1]
fashion_mnist-labels[2]
fashion_mnist_labels[2]
class_names[2]
class_names[1]
class_names[1, 1]
class_names[2, 1]
class_names[2, 3]
class_names[2,]
class_names[2,1]
class_names[2,2]
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2, labels = c(class_names[g_train[1],2]))
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1],2],
class_names[g_train[2],2],
class_names[g_train[3],2],
class_names[g_train[4],2],
class_names[g_train[5],2],
class_names[g_train[6],2]))
class_names
class_names[0,2]
class_names[1,2]
class_names[5,2]
g_train[1]
g_train[3]
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]))
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]),
label_colour = "blue")
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]),
label_x = 0)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]),
label_x = -0.5)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]),
label_x = -0.1)
model_lr.h5 <- model_lr %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 5,               # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
model_lr_hist.RDS <- model_lr.h5$history$history
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
history = model_lr %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 5,               # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
```{r}
history = model_lr %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 5,               # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
history = model_lr %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 10,               # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
model_lr <- load_model_hdf5("model_lr.h5")
model_lr_hist <- readRDS("model_lr_hist.RDS")
plot_model_history(model_lr_hist)
history = model_lr %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 10,               # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
save_model_hdf5(model_lr, "model_lr.h5") # Save Model
saveRDS(model_lr$history$history, "model_lr_hist.RDS") # Save History
model_lr <- load_model_hdf5("model_lr.h5")
model_lr_hist <- readRDS("model_lr_hist.RDS")
plot_model_history(model_lr_hist)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2,
labels = c(class_names[g_train[1] + 1,2],
class_names[g_train[2] + 1,2],
class_names[g_train[3] + 1,2],
class_names[g_train[4] + 1,2],
class_names[g_train[5] + 1,2],
class_names[g_train[6] + 1,2]),
label_x = -0.1)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2),
p6 <- plot_grayscale(x_train[6,,,])
p1 <- plot_grayscale(x_train[1,,,], g_train[1], class_names)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
p1 <- plot_grayscale(x_train[1,,,], g_train[1], class_names)
p2 <- plot_grayscale(x_train[2,,,], g_train[2], class_names)
p3 <- plot_grayscale(x_train[3,,,], g_train[3], class_names)
p4 <- plot_grayscale(x_train[4,,,], g_train[4], class_names)
p5 <- plot_grayscale(x_train[5,,,], g_train[5], class_names)
p6 <- plot_grayscale(x_train[6,,,], g_train[6], class_names)
plot_grid(p1, p2, p3, p4, p5, p6, nrow = 2)
model_nn = keras_model_sequential() %>%
layer_flatten(input_shape = c(img_rows, img_cols, 1)) %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 10, activation = "softmax")
model_nn %>% compile(loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)
summary(model_nn)
history = model_nn %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 15,              # an epoch is a gradient step
batch_size = 128,         # we will learn about batches in Lecture 2
validation_split = 0.2)
# save model
save_model_hdf5(model_nn, "model_nn.h5")
# save history
saveRDS(model_nn$history$history, "model_nn_hist.RDS")
# load model
model_nn = load_model_hdf5("model_nn.h5")
# load history
model_nn_hist = readRDS("model_nn_hist.RDS")
plot_model_history(model_nn_hist)
model_cnn <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = c(img_rows, img_cols, 1)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
model_cnn <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = c(img_rows, img_cols, 1)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
model_cnn %>% compile(loss = "categorical_crossentropy",
optimizer = optimizer_adadelta(),
metrics = c("accuracy")
)
summary(model_cnn)
```{r, eval = FALSE}
model_cnn %>%
fit(x_train,                  # supply training features
y_train,                  # supply training responses
epochs = 8,               # an epoch cycles through all mini-batches
batch_size = 128,         # mini-batch size
validation_split = 0.2)   # use 20% of the training data for validation
# save model
save_model_hdf5(model_cnn, "model_cnn.h5")
# save history
saveRDS(model_cnn$history$history, "model_cnn_hist.RDS")
plot_model_history(model_cnn_hist)
# load model
model_cnn = load_model_hdf5("model_cnn.h5")
# load history
model_cnn_hist = readRDS("model_cnn_hist.RDS")
plot_model_history(model_cnn_hist)
predicted_classes_lr <- model_lr %>%
predict(x_test) %>%
k_argmax() %>%
as.integer()
predicted_classes_nn <- model_nn %>%
predict(x_test) %>%
k_argmax() %>%
as.integer()
predicted_classes_cnn <- model_cnn %>%
predict(x_test) %>%
k_argmax() %>%
as.integer()
# Get the accuracy of each of the three models
accuracy_lr <- mean(predicted_classes_lr == g_test)
accuracy_nn <- mean(predicted_classes_nn == g_test)
accuracy_cnn <- mean(predicted_classes_cnn == g_test)
as_tibble(data.frame(
"Model" = c("Human", "MC_Log_Reg", "FC_NN", "CNN"),
"Num_Layers" = c(0, 1, 4, 4),
"Num_Params" = c(0, 7850, 242762, 1199882),
"Ms_per_stp" = c(0, 3, 8, 190),
"Accuracy" = c(0.835, accuracy_lr, accuracy_nn, accuracy_cnn)))
as_tibble(data.frame(
"Model" = c("Human", "MC_Log_Reg", "FC_NN", "CNN"),
"Num_Layers" = c(0, 1, 4, 4),
"Num_Params" = c(0, 7850, 242762, 1199882),
"Ms_per_stp" = c(0, 3, 8, 190),
"Accuracy" = c(0.835, accuracy_lr, accuracy_nn, accuracy_cnn))) %>%
kable(row.names = NA,
booktabs = TRUE,
digits = 2) %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
as_tibble(data.frame(
"Model" = c("Human", "MC_Log_Reg", "FC_NN", "CNN"),
"Num_Layers" = c(0, 1, 4, 4),
"Num_Params" = c(0, 7850, 242762, 1199882),
"Ms_per_stp" = c(0, 3, 8, 190),
"Accuracy" = c(0.835, accuracy_lr, accuracy_nn, accuracy_cnn))) %>%
kable(row.names = NA,
booktabs = TRUE,
digits = 3) %>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
plot_confusion_matrix(predicted_responses = predicted_classes_lr,
actual_response = g_test)
plot_confusion_matrix(predicted_responses = predicted_classes_nn,
actual_response = g_test)
plot_confusion_matrix(predicted_responses = predicted_classes_cnn,
actual_response = g_test)
pp <- match(6, g_test)
pp
pp <- which(6, g_test)
pp <- which(6 %in% g_test)
pp
g_test
pp <- which(6 %in% g_test)
pp
pp <- which(g_test %in% 6)
pp
predicted_responses_cnn
predicted_classes_cnn
predicted_classes_cnn[pp] != 6
predicted_classes_cnn
predicted_classes_cnn
predicted_classes_cnn[pp]
match(c(0, 2, 4), predicted_classes_cnn[pp])
match(c(0, 2, 4), predicted_classes_cnn[pp])
g_test[54]
g_test[55]
g_test[53]
g_test[54]
g_test[20]
g_test[19]
pp <- g_test == 6
pp
match(c(0, 2, 4), predicted_classes_cnn[pp])
predicted_classes_cnn[pp]
match(c(0, 2, 4), predicted_classes_cnn[pp])
pp <- which(g_test %in% 6)
pp
predict_zeros <- which(predicted_classes_cnn %in% 0)
predict_zeros <- which(predicted_classes_cnn %in% 0)
index_top <- predicted_clases_cnn[test_sixes == predicts_zeros][1]
index_top <- predicted_classes_cnn[test_sixes == predicts_zeros][1]
test_sixes <- which(g_test %in% 6)
predict_zeros <- which(predicted_classes_cnn %in% 0)
index_top <- predicted_classes_cnn[test_sixes == predicts_zeros][1]
index_top <- predicted_classes_cnn[test_sixes == predict_zeros][1]
index_top
test_sixes == predict_zeros
test_sixes[54]
g_test[566]
predicted_classes_cnn[566]
top, pull, coat <- match(c(0, 2, 4), predicted_classes_cnn[pp])
[top, pull, coat] <- match(c(0, 2, 4), predicted_classes_cnn[pp])
indices <- match(c(0, 2, 4), predicted_classes_cnn[pp])
top_ind <- test_sixes[indices[1]]
top_ind
pullover_ind <- test_sixes[indices[2]]
coat_ind <- test_sixes[indices[3]]
test_sixes <- which(g_test %in% 6)
indices <- match(c(0, 2, 4), predicted_classes_cnn[pp])
top_ind <- test_sixes[indices[1]]
pullover_ind <- test_sixes[indices[2]]
coat_ind <- test_sixes[indices[3]]
p1 <- plot_grayscale(x_test[top_ind,,,],
g_test[top_ind], class_names)
p2 <- plot_grayscale(x_test[pullover_ind,,,],
g_test[pullover_ind], class_names)
p3 <- plot_grayscale(x_test[coat_ind,,,],
g_test[coat_ind], class_names)
plot_grid(p1, p2, p3)
plot_grid(p1, p2, p3, nrow = 1)
p1 <- plot_grayscale(x_test[top_ind,,,],
predicted_classes_cnn[top_ind], class_names)
p2 <- plot_grayscale(x_test[pullover_ind,,,],
predicted_classes_cnn[pullover_ind], class_names)
p3 <- plot_grayscale(x_test[coat_ind,,,],
predicted_classes_cnn[coat_ind], class_names)
plot_grid(p1, p2, p3, nrow = 1)
setwd("~/Stat471/uk-census-health")
# create lasso CV plot
ggsave(filename = "results/lasso-cv-plot.png",
plot = plot(lasso_fit),
device = "png",
width = 6,
height = 4)
# load libraries
library(kableExtra)
library(glmnetUtils)  # to run ridge and lasso
source("code/functions/plot_glmnet.R") # for lasso/ridge trace plots
source("code/functions/get_misclass_errors.R")
# read in the training data
census_train = read_csv("data/clean/census_train.csv")
census_test = read_csv("data/clean/census_test.csv")
# create lasso CV plot
ggsave(filename = "results/lasso-cv-plot.png",
plot = plot(lasso_fit),
device = "png",
width = 6,
height = 4)
library(tidyverse)
# create ridge CV plot
ggsave(filename = "results/ridge-cv-plot.png",
plot = plot(ridge_fit),
device = "png",
width = 6,
height = 4)
ridge_fit<-load("results/ridge_fit.Rda")
ridge_probabilities = predict(ridge_fit,
newdata = census_test,
s = "lambda.1se",
type = "response") %>%
as.numeric()
ridge_model_metrics <- get_misclass_errors(ridge_probabilities, census_test)
# read in the training data
census_train = read_csv("data/clean/census_train.csv")
census_test = read_csv("data/clean/census_test.csv")
# Run Univariate logistic regression using Age
glm_fit_age <- glm(Health ~ Age, family = 'binomial', data = census_train)
summary(glm_fit_age)
save(glm_fit_age, file = "results/glm_fit_age.Rda")
glm_fit_full <- glm(`Health` ~., family = 'binomial', data = census_train)
coef(glm_fit_full)
fitted_probabilities <- predict(glm_fit_full,
newdata = census_test,
type = "response")
glm_model_metrics <- get_misclass_errors(fitted_probabilities, census_test)
save(glm_fit_full, file = "results/glm_fit_full.Rda")
fitted_probabilities <- predict(glm_fit_full,
newdata = census_test,
type = "response")
glm_model_metrics <- get_misclass_errors(fitted_probabilities, census_test)
write_csv(x = glm_model_metrics, file = "data/clean/glm-metrics-table.csv")
# Ridge penalised logistic regression
set.seed(5)
ridge_fit = cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
# save the ridge fit object
save(ridge_fit, file = "results/ridge_fit.Rda")
ridge_probabilities = predict(ridge_fit,
newdata = census_test,
s = "lambda.1se",
type = "response") %>%
as.numeric()
ridge_model_metrics <- get_misclass_errors(ridge_probabilities, census_test)
ridge_model_metrics
write_csv(x = ridge_model_metrics, file = "data/clean/ridge-metrics-table.csv")
# create ridge CV plot
ggsave(filename = "results/ridge-cv-plot.png",
plot = plot(ridge_fit),
device = "png",
width = 6,
height = 4)
# create ridge trace plot
ggsave(filename = "results/ridge-trace-plot.png",
plot = plot_glmnet(ridge_fit,
census_train,
features_to_plot = 6),
device = "png",
width = 6,
height = 4)
pp <- plot(ridge_fit)
pp
library(glmnet)
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
# Ridge penalised logistic regression
set.seed(5)
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
ridge_probabilities <- predict(ridge_fit,
newdata = census_test,
s = "lambda.1se",
type = "response") %>%
as.numeric()
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
library(glmnet)
library(glmnetUtils)  # to run ridge and lasso
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
library(glmnetUtils)  # to run ridge and lasso
source("code/functions/get_misclass_errors.R")
ridge_fit <- cv.glmnet(`Health` ~ .,
alpha = 0,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
glm_fit_full <- glm(`Health` ~., family = 'binomial', data = census_train)
lasso_fit = cv.glmnet(`Health` ~ .,
alpha = 1,
nfolds = 10,
family = 'binomial',
type.measure = 'class',
data = census_train)
